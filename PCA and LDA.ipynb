{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis is a linear transformation technique that has applications in several domains, most commonly in dimensionality reduction and data compression. PCA identifies the correlation between the features and tries to remove the redundant or less useful features. In essence, it finds the subspace that can best represent the high dimensional data. Some other commonly used dimensionality reduction techniques are - Linear Discriminant Analysis (LDA), t-SNE and Autoencoders.\n",
    "\n",
    "I have implemented both PCA and LDA from scratch. Using the PCA technique, I also demonstrate the reconstruction of the original images from the sum of first k principal components. Similar experimenting with LDA has also been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "import scipy\n",
    "import time\n",
    "from math import sqrt, cos, pi\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_LDA_SNE_AEN_Analysis:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.train_data = []\n",
    "        self.train_labels = []\n",
    "        self.test_data = []\n",
    "        self.test_labels = []\n",
    "        self.data_classes = {}\n",
    "    \n",
    "    def unpickle(self, file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load the CIFAR dataset, there are 5 batches of training images, let's load\n",
    "        # each of them and append them together\n",
    "        print('loading data of batch: ')\n",
    "        for i in range(5):\n",
    "            print(str(i+1))\n",
    "            this_data = self.unpickle('dataset_CIFAR/data_batch_'+str(i+1))\n",
    "            # the dictionary has the following elements - batch_label, labels, data and filenames\n",
    "            # we only need the data and the labels\n",
    "            if i == 0:\n",
    "                self.train_data = this_data[b'data']\n",
    "                self.train_labels = this_data[b'labels']\n",
    "                continue\n",
    "            self.train_data = np.vstack((self.train_data, this_data[b'data']))\n",
    "            self.train_labels.extend(this_data[b'labels'])\n",
    "        # now load the test data\n",
    "        test_all_data = self.unpickle('dataset_CIFAR/test_batch')\n",
    "        self.test_data = test_all_data[b'data']\n",
    "        self.test_labels = test_all_data[b'labels']\n",
    "        # this data_class dict object in turn has following elements - num_cases_per_batch,\n",
    "        # label_names and num_vis. We may need only label_names mostly, hence taking only that\n",
    "        self.data_classes = self.unpickle('dataset_CIFAR/batches.meta')[b'label_names']\n",
    "        return\n",
    "\n",
    "    def histo_visual(self):\n",
    "        with plt.style.context('seaborn-whitegrid'):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            #visualize the first ten features for all categories\n",
    "            colors = cm.rainbow(np.linspace(0, 1, 10))\n",
    "            for cnt in range(10):\n",
    "                print('feature '+ str(cnt+1))\n",
    "                plt.subplot(5, 2, cnt+1)\n",
    "                for lab in range(len(self.data_classes)):\n",
    "                    # indi = np.where(np.array(self.train_labels)==lab)\n",
    "                    indi = [i for i in range(len(self.train_labels)) if self.train_labels[i] == lab]\n",
    "                    plt.hist(self.train_data[indi, cnt],\n",
    "                             bins=10,\n",
    "                             )\n",
    "                plt.xlabel('feature ' + str(cnt+1))\n",
    "            plt.legend(loc='upper right', fancybox=True, fontsize=8)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_f(self):\n",
    "        self.load_data()\n",
    "        '''#check to see if 50000 training images and 10000 test images with labels have been loaded\n",
    "        print(self.train_data.shape)\n",
    "        print(len(self.train_labels))\n",
    "        print(self.test_data.shape)\n",
    "        print(len(self.test_labels))\n",
    "        print(self.data_classes)'''\n",
    "        # plot a histogram of the first 10 features\n",
    "        self.histo_visual()\n",
    "        '''# standardize the image:\n",
    "        self.train_data = StandardScaler().fit_transform(self.train_data)\n",
    "        mean_vec = np.mean(self.train_data, axis=0)\n",
    "        covar = (self.train_data - mean_vec).T.dot((self.train_data - mean_vec)) / (self.train_data.shape[0]-1)'''\n",
    "        covar = np.cov(self.train_data.T)\n",
    "        print('Covariance matrix \\n ', covar)\n",
    "        print('shape of covar matrix ', covar.shape)\n",
    "\n",
    "        #option 1 - eigen value decomposition\n",
    "        eigv, eig_vec = np.linalg.eig(covar)\n",
    "\n",
    "        #option 2 - singular value decomposition\n",
    "        '''u,s,v = scipy.linalg.svd(self.train_data.T.astype(np.float32))\n",
    "        print(v)'''\n",
    "\n",
    "        #(eigen value, eigen vector) tuples\n",
    "        eigen_all = [(np.abs(eigv[i]), eig_vec[:,i]) for i in range(len(eigv))]\n",
    "        # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "        eigen_all.sort(key=lambda y: y[0], reverse=True)\n",
    "\n",
    "        # check the sorted order\n",
    "        '''print('Sorted top 10 Eigenvalues:')\n",
    "        k = 0\n",
    "        for j in eigen_all:\n",
    "            print(j[0])\n",
    "            k+=1\n",
    "            if k>= 10 :\n",
    "                break'''\n",
    "        # determine the value of K for selecting the first k principal components\n",
    "        eigv_sum = sum(eigv)\n",
    "        variance_ = [(i / eigv_sum)*100 for i in sorted(eigv, reverse=True)]\n",
    "        print(variance_[:10])\n",
    "        cum_var = np.cumsum(variance_)\n",
    "\n",
    "        with plt.style.context('seaborn-whitegrid'):\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            '''plt.bar(range(len(eigv)), variance_, alpha=0.5, align='center',\n",
    "                    label='individual explained variance')\n",
    "            plt.step(range(len(eigv)), cum_var, where='mid',\n",
    "                     label='cumulative explained variance')'''\n",
    "            plt.bar(range(10), variance_[:10], alpha=0.5, align='center',\n",
    "                    label='individual explained variance')\n",
    "            plt.step(range(10), cum_var[:10], where='mid',\n",
    "                     label='cumulative explained variance')\n",
    "            plt.ylabel('Explained variance ratio')\n",
    "            plt.xlabel('Principal components')\n",
    "            plt.legend(loc='best')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        # plot the first 2 principal components and visualize the data separation\n",
    "        # (use inbuilt pca from scikit to fit and then visualize)\n",
    "        \n",
    "        # show the original images (one image from each of the 10 categories)\n",
    "        num_eg = 5\n",
    "        indi = [np.where(np.array(self.train_labels)==lab)[:num_eg] for lab in range(0, len(self.data_classes))]\n",
    "        # plotting the images\n",
    "        fig, aa = plt.subplots(len(self.data_classes),num_eg,figsize=(32, 32))\n",
    "        for class_id in range(len(self.data_classes)):\n",
    "            for i in range(num_eg):\n",
    "                #aa[class_id,i].imshow(np.reshape(self.train_data[indi[class_id][0][i]],(32,32,3)))\n",
    "                aa[class_id, i].imshow(np.dstack((self.train_data[indi[class_id][0][i]][:1024].reshape((32,32,1)),self.train_data[indi[class_id][0][i]][1024:2048].reshape((32,32,1)),self.train_data[indi[class_id][0][i]][2048:].reshape((32,32,1)))))\n",
    "                aa[class_id,i].axis('off')\n",
    "\n",
    "        fig.suptitle('{} sample images from each class'.format(5))\n",
    "        plt.show()\n",
    "        # reconstruct the image using first 10 principal components (of the same 10 images):\n",
    "        # find mean image from training data set:\n",
    "        train_data_mean = np.mean(self.train_data, axis=0)\n",
    "        mean_sub_data = self.train_data - train_data_mean\n",
    "        # now reconstruct the data for each image with the 10 eigen vectors\n",
    "        coeff = []\n",
    "        top_ten_eig = []\n",
    "        for i in range(200):\n",
    "            this_coeff = np.matmul(mean_sub_data, eigen_all[i][1])\n",
    "            if i==0:\n",
    "                coeff = this_coeff\n",
    "                top_ten_eig = eigen_all[i][1]\n",
    "                continue\n",
    "            coeff = np.vstack((coeff, this_coeff))\n",
    "            top_ten_eig = np.vstack((top_ten_eig, eigen_all[i][1]))\n",
    "        # shape of coeff was 200 x 50000, now changing it to 50,000 x 200\n",
    "        coeff = coeff.T\n",
    "        print(coeff.shape)\n",
    "        #shape of top_ten_eig is 200 x 3072\n",
    "        recon_img = np.matmul(coeff,top_ten_eig)\n",
    "        recon_img += train_data_mean\n",
    "        print(recon_img.shape)\n",
    "        #plot reconstructed Images\n",
    "        num_eg = 5\n",
    "        indi = [np.where(np.array(self.train_labels)==lab)[:num_eg] for lab in range(0, len(self.data_classes))]\n",
    "        # plotting the images\n",
    "        fig, aa = plt.subplots(len(self.data_classes),num_eg,figsize=(32, 32))\n",
    "        for class_id in range(len(self.data_classes)):\n",
    "            for i in range(num_eg):\n",
    "                aa[class_id, i].imshow(np.dstack((recon_img[indi[class_id][0][i]][:1024].reshape((32,32,1)),recon_img[indi[class_id][0][i]][1024:2048].reshape((32,32,1)),recon_img[indi[class_id][0][i]][2048:].reshape((32,32,1)))).astype(np.uint8))\n",
    "                aa[class_id,i].axis('off')\n",
    "\n",
    "        fig.suptitle('{} reconstructed images from each class'.format(5))\n",
    "        plt.show()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_f(self):\n",
    "        self.load_data()\n",
    "        # first, let's compute the mean vectors of each class\n",
    "        mean_vectors = []\n",
    "        for cl in range(10):\n",
    "            indi = [np.where(np.array(self.train_labels)==cl)]\n",
    "            mean_vectors.append(np.mean(np.reshape(self.train_data[indi], (self.train_data[indi].shape[1], self.train_data[indi].shape[2])), axis=0))\n",
    "            print('Mean Vector class ', cl, ': ', mean_vectors[cl], '\\n')\n",
    "            \n",
    "        # compute within class scatter matrix\n",
    "        scatter_within = np.zeros((3072, 3072))\n",
    "        for lab, meanv in zip(range(10), mean_vectors):\n",
    "            print('label ', lab)\n",
    "            scat_matrix = np.zeros((3072, 3072))\n",
    "            indi = [np.where(np.array(self.train_labels)==lab)]\n",
    "            meanv = meanv.reshape(3072, 1)\n",
    "            for data in np.reshape(self.train_data[indi], (self.train_data[indi].shape[1], self.train_data[indi].shape[2])):\n",
    "                data = data.reshape(3072,1)\n",
    "                scat_matrix = np.dot(data-meanv, (data-meanv).T)\n",
    "                #scat_matrix += (data-meanv).dot((data-meanv).T)\n",
    "            scatter_within += scat_matrix\n",
    "            \n",
    "        print('within-class Scatter Matrix:\\n', scatter_within)\n",
    "\n",
    "        # compute between class scatter matrix\n",
    "        overall_mean = np.mean(self.train_data, axis=0)\n",
    "        scatter_between = np.zeros((3072, 3072))\n",
    "        for lab,meanv in enumerate(mean_vectors):\n",
    "            indi = [np.where(np.array(self.train_labels)==lab)]\n",
    "            n = np.reshape(self.train_data[indi], (self.train_data[indi].shape[1], self.train_data[indi].shape[2])).shape[0]\n",
    "            meanv = meanv.reshape(3072,1)\n",
    "            overall_mean = overall_mean.reshape(3072,1)\n",
    "            scatter_between += n * np.dot((meanv - overall_mean),(meanv - overall_mean).T)\n",
    "        print('between-class Scatter Matrix:\\n', scatter_between)\n",
    "\n",
    "        #EVD:\n",
    "        eigv, eig_vec = np.linalg.eig(np.linalg.inv(np.dot(scatter_within, scatter_between)))\n",
    "\n",
    "        ##Rest is the same as PCA \n",
    "        # print 10 eigen vectors\n",
    "        for i in range(10):\n",
    "            eigvec_sc = eig_vec[:,i].reshape(3072,1)   \n",
    "            print('\\nEigenvector {}: \\n{}'.format(i+1, eigvec_sc.real))\n",
    "            print('Eigenvalue {:}: {:.2e}'.format(i+1, eigv[i].real))\n",
    "\n",
    "        #(eigen value, eigen vector) tuples\n",
    "        eigen_all = [(np.abs(eigv[i]), eig_vec[:,i]) for i in range(len(eigv))]\n",
    "        # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "        eigen_all.sort(key=lambda y: y[0], reverse=True)\n",
    "\n",
    "        # check the sorted order\n",
    "        '''print('Sorted top 10 Eigenvalues:')\n",
    "        k = 0\n",
    "        for j in eigen_all:\n",
    "            print(j[0])\n",
    "            k+=1\n",
    "            if k>= 10 :\n",
    "                break'''\n",
    "\n",
    "        print('Variance explained:\\n')\n",
    "        eigv_sum = sum(eigv)\n",
    "        for i,j in enumerate(eigen_all):\n",
    "            print('eigenvalue {0:}: {1:.2%}'.format(i+1, (j[0]/eigv_sum).real))\n",
    "\n",
    "        with plt.style.context('seaborn-whitegrid'):\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            '''plt.bar(range(len(eigv)), variance_, alpha=0.5, align='center',\n",
    "                    label='individual explained variance')\n",
    "            plt.step(range(len(eigv)), cum_var, where='mid',\n",
    "                     label='cumulative explained variance')'''\n",
    "            plt.bar(range(10), variance_[:10], alpha=0.5, align='center',\n",
    "                    label='individual explained variance')\n",
    "            plt.step(range(10), cum_var[:10], where='mid',\n",
    "                     label='cumulative explained variance')\n",
    "            plt.ylabel('Explained variance ratio')\n",
    "            plt.xlabel('Principal components')\n",
    "            plt.legend(loc='best')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        # plot the first 2 principal components and visualize the data separation\n",
    "        # (use inbuilt pca from scikit to fit and then visualize)\n",
    "        \n",
    "        # show the original images (one image from each of the 10 categories)\n",
    "        num_eg = 5\n",
    "        indi = [np.where(np.array(self.train_labels)==lab)[:num_eg] for lab in range(0, len(self.data_classes))]\n",
    "        # plotting the images\n",
    "        fig, aa = plt.subplots(len(self.data_classes),num_eg,figsize=(32, 32))\n",
    "        for class_id in range(len(self.data_classes)):\n",
    "            for i in range(num_eg):\n",
    "                #aa[class_id,i].imshow(np.reshape(self.train_data[indi[class_id][0][i]],(32,32,3)))\n",
    "                aa[class_id, i].imshow(np.dstack((self.train_data[indi[class_id][0][i]][:1024].reshape((32,32,1)),self.train_data[indi[class_id][0][i]][1024:2048].reshape((32,32,1)),self.train_data[indi[class_id][0][i]][2048:].reshape((32,32,1)))))\n",
    "                aa[class_id,i].axis('off')\n",
    "\n",
    "        fig.suptitle('{} sample images from each class'.format(5))\n",
    "        plt.show()\n",
    "        # reconstruct the image using first 10 principal components (of the same 10 images):\n",
    "        # find mean image from training data set:\n",
    "        train_data_mean = np.mean(self.train_data, axis=0)\n",
    "        mean_sub_data = self.train_data - train_data_mean\n",
    "        # now reconstruct the data for each image with the 10 eigen vectors\n",
    "        coeff = []\n",
    "        top_ten_eig = []\n",
    "        for i in range(200):\n",
    "            this_coeff = np.matmul(mean_sub_data, eigen_all[i][1])\n",
    "            if i==0:\n",
    "                coeff = this_coeff\n",
    "                top_ten_eig = eigen_all[i][1]\n",
    "                continue\n",
    "            coeff = np.vstack((coeff, this_coeff))\n",
    "            top_ten_eig = np.vstack((top_ten_eig, eigen_all[i][1]))\n",
    "        # shape of coeff was 10 x 50000, now changing it to 50,000 x 10\n",
    "        coeff = coeff.T\n",
    "        print(coeff.shape)\n",
    "        #shape of top_ten_eig is 10 x 3072\n",
    "        recon_img = np.matmul(coeff,top_ten_eig)\n",
    "        recon_img += train_data_mean\n",
    "        print(recon_img.shape)\n",
    "        #plot reconstructed Images\n",
    "        num_eg = 5\n",
    "        indi = [np.where(np.array(self.train_labels)==lab)[:num_eg] for lab in range(0, len(self.data_classes))]\n",
    "        # plotting the images\n",
    "        fig, aa = plt.subplots(len(self.data_classes),num_eg,figsize=(32, 32))\n",
    "        for class_id in range(len(self.data_classes)):\n",
    "            for i in range(num_eg):\n",
    "                aa[class_id, i].imshow(np.dstack((recon_img[indi[class_id][0][i]][:1024].reshape((32,32,1)),recon_img[indi[class_id][0][i]][1024:2048].reshape((32,32,1)),recon_img[indi[class_id][0][i]][2048:].reshape((32,32,1)))).astype(np.uint8))\n",
    "                aa[class_id,i].axis('off')\n",
    "\n",
    "        fig.suptitle('{} reconstructed images from each class'.format(5))\n",
    "        plt.show()\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(self, choice):\n",
    "        options = {'pca':self.pca_f,\n",
    "                   'lda':self.lda_f,\n",
    "            }\n",
    "        options[choice]()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # creating an instance of the PCA class\n",
    "    pca_class = PCA_LDA_SNE_AEN_Analysis()\n",
    "\n",
    "    # Call to the main function\n",
    "    # Arguments to the main function is the analysis type (PCA, LDA, t-SNE or Auto encoders)\n",
    "    tic = time.time()\n",
    "    print('Choices are : \\n 1. PCA (pca), \\n 2. lda (lda_f), \\n 3. t_sne visualization (t_sne) and \\n 4. Auto-encoders(aen)')\n",
    "    choice_v = input(\"Enter the choice: \")\n",
    "    pca_class.main(choice_v)\n",
    "    toc = time.time() - tic\n",
    "    print(\"Running time: \" + str(toc))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that was used for this exercise is the CIFAR-10 dataset. This dataset has in total 60000 images of size (32, 32, 3) (RGB image), out of which 10000 images are test images. The different categories of the dataset are :\n",
    "Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship and truck.\n",
    "\n",
    "There are around 5000 training images for each category. The image is flattened into a vector, so the total number of features is 32 x 32 x 3 = 3072.\n",
    "\n",
    "The steps of PCA can be summarized as :\n",
    "1. Take the covariance matrix (of features) and perform eigen value decomposition to find the eigen vectors and eigen values. SVD can also be done in place of EVD\n",
    "2. Sort the eigen values in decreasing order and select the first $\\mathrm{k}$ principal components. The value of $\\mathrm{k}$ can be determined based on the percentage variance and identifying those that have high variance.\n",
    "3. Now, take the entire image data containing feature vectors $\\textbf{x}$ and subtract from it the mean image $\\textit{x}$ \n",
    "4. Find the $\\mathrm{k}$ coefficients for a given feature vector $\\textbf{x}$ as $\\phi_{k} = (\\textbf{x}-\\textit{x}).v_{k}$ where $v_{k}$ is the $\\mathrm{k}^{th}$ principal vector.\n",
    "5. Now reconstruct the image as: $\\textbf{x} = \\sum_{j=1}^{k}\\phi_{k}v_{k}$.\n",
    "\n",
    "\n",
    "The above steps have been clearly implemented in the code and we now discuss the results.\n",
    "\n",
    "Visualization of the distribution of the first 10 features within the 10 different categories/labels. \n",
    "![Alt text](imgs/histo.PNG?raw=true \"histo\")\n",
    "\n",
    "Sample images from each category :\n",
    "![Alt text](imgs/original_images.PNG?raw=true \"original_images\")\n",
    "\n",
    "Now we see the plot of percentage variance of the different eigenvectors:\n",
    "![Alt text](imgs/variance_ex.png?raw=true \"variance_ex\")\n",
    "\n",
    "As we can see, for the CIFAR-10 dataset, the first 10 eigenvectors contribute to ~60% of the variance in data. This means that the new 10-dimensional subspace represents 60% of the original data. However, during reconstruction I found that nearly 200 principal components produced effective reproduction of the original images with comparatively lesser loss. The reconstructed images are :\n",
    "\n",
    "![Alt text](imgs/200_pc_recon_32_1.PNG?raw=true \"200_pc_recon_32_1\")\n",
    "![Alt text](imgs/200_pc_recon_32_2.PNG?raw=true \"200_pc_recon_32_2\")\n",
    "\n",
    "100 principal components used :\n",
    "\n",
    "![Alt text](imgs/pca_recon_100.PNG?raw=true \"pca_recon_100\")\n",
    "\n",
    "200 principal components used:\n",
    "\n",
    "![Alt text](imgs/200_pc_recon_Fin.PNG?raw=true \"200_pc_recon_Fin\")\n",
    "\n",
    "We can clearly see that the reconstructed image is of reasonable quality. Also, though there is loss of information in the images, the loss improves as more principal components are added. \n",
    "\n",
    "**LDA :**\n",
    "The primary goal of LDA is to provide maximum separation between the data of different classes. So, while PCA works in an unsupervised setting and is unmindful of the labels of the data, LDA works with these labels and tries to find that subspace that can best separate the data of different categories.  \n",
    "The procedure can be outlined as :\n",
    "1. Find the mean vectors for each category of the data\n",
    "2. Find the between class and within class scatter matrices\n",
    "3. Find the eigenvectors and eigenvalues of the $\\left(\\mathrm{Scatter}_{within}\\right)^{-1}\\times \\mathrm{Scatter}_{between}$\n",
    "4. Follow steps 2 to 4 of PCA \n",
    "\n",
    "The major variation here is that the eigenvectors and eigen values computed with the scatter matrices show that the first 10 eigenvectors contribute to ~100% of the variance. This is a large difference compared to the analysis from PCA. I tried reconstructing the image and visualizing it, but it was not successful. \n",
    "\n",
    "![Alt text](imgs/lda_output_2.PNG?raw=true \"lda_output_2\")\n",
    "\n",
    "One comparison that can be done between the PCA and LDA is, plotting the data to see how well the first 2 components separate the data. That is, we reduce the data to 2-dimensions by transforming the original data to the new eigenspace and see how well they are separated. We do the same for LDA.\n",
    "**PCA Output**\n",
    "![Alt text](imgs/PCA_Plot.png?raw=true \"PCA_Plot\")\n",
    "**LDA Output**\n",
    "![Alt text](imgs/LDA_Plot.png?raw=true \"LDA_Plot\")\n",
    "\n",
    "We can observe that both PCA and LDA provide similar separation between the classes and it is hard to observe which one is best. However, in LDA the number of linear discriminants is utmost (number of classes - 1) and we can observe in our result also that only the first 9 eigen vectors sum up to produce cumulative variance of 100%\n",
    "\n",
    "I have only done a simple analysis and there is more scope to explore the performance of these two and also other such algorithms\n",
    "\n",
    "#TODO: \n",
    "1. Plot of eigen images\n",
    "2. Reduction in loss as the number of principal components is increased"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
